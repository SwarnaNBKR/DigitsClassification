{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\swarn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\swarn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\swarn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\swarn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\swarn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\swarn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\swarn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\swarn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\swarn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: torchvision in c:\\users\\swarn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.20.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\swarn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: torch==2.5.1 in c:\\users\\swarn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (2.5.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\swarn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\swarn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.5.1->torchvision) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\swarn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\swarn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.5.1->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\swarn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.5.1->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\swarn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.5.1->torchvision) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\swarn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\swarn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\swarn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install torch\n",
    "! pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data= datasets.MNIST( root=\"data\", train=True, download=True, transform=ToTensor(),)\n",
    "test_data=datasets.MNIST( root=\"data\", train=False, download=True, transform=ToTensor(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.mnist.MNIST"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.mnist.MNIST"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N,C,H, W]:torch.Size([64, 1, 28, 28])\n",
      "Shape of y:torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "\n",
    "train_dataloader=DataLoader(training_data,batch_size=batch_size)\n",
    "test_dataloader=DataLoader(test_data,batch_size=batch_size)\n",
    "\n",
    "for X,y in test_dataloader:\n",
    "    print(f\"Shape of X [N,C,H, W]:{X.shape}\")\n",
    "    print(f\"Shape of y:{y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def show_image(image, label):\n",
    "    plt.imshow(image.squeeze(),cmap='gray')\n",
    "    plt.title(f\"label:{label}\")\n",
    "    plt.show()\n",
    "show_image(X[0],y[0])\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
       "        4, 0, 7, 4, 0, 1, 3, 1, 3, 4, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2, 3, 5, 1, 2,\n",
       "        4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 9, 3])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.33 0.73 0.62 0.59 0.24 0.14 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.87 1.00 1.00 1.00 1.00 0.95 0.78 0.78 0.78 0.78 0.78 0.78 0.78 0.78 0.67 0.20 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.26 0.45 0.28 0.45 0.64 0.89 1.00 0.88 1.00 1.00 1.00 0.98 0.90 1.00 1.00 0.55 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.07 0.26 0.05 0.26 0.26 0.26 0.23 0.08 0.93 1.00 0.42 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.33 0.99 0.82 0.07 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.09 0.91 1.00 0.33 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.51 1.00 0.93 0.17 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.23 0.98 1.00 0.24 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.52 1.00 0.73 0.02 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.04 0.80 0.97 0.23 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.49 1.00 0.71 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.29 0.98 0.94 0.22 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.07 0.87 1.00 0.65 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.80 1.00 0.86 0.14 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.15 1.00 1.00 0.30 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.12 0.88 1.00 0.45 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.52 1.00 1.00 0.20 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.24 0.95 1.00 1.00 0.20 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.47 1.00 1.00 0.86 0.16 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.47 1.00 0.81 0.07 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n"
     ]
    }
   ],
   "source": [
    "x=X[0].squeeze()\n",
    "for row in x:\n",
    "    for item in row:\n",
    "        print(\"{:.2f}\".format(item.item()),end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n"
     ]
    }
   ],
   "source": [
    "x=X[0].squeeze()\n",
    "for row in x:\n",
    "    for item in row:\n",
    "        pixel=0\n",
    "        if item.item()>0:\n",
    "            pixel=1\n",
    "        print(pixel,end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu device\n"
     ]
    }
   ],
   "source": [
    "# get gpu,cpu, or mps device for training\n",
    "device=(\n",
    "    \"cuda\" \n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\")\n",
    "print(f\"using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "x = torch.randn(1,28,28)\n",
    "p1= nn.Flatten()(x) # .to(device)\n",
    "print(p1.shape)\n",
    "p2=nn.Linear(28*28,512)(p1)\n",
    "print(p2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.linear_relu_stack=nn.Sequential(\n",
    "            nn.Linear(28*28,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,10)\n",
    "          )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0085,  0.0344, -0.0747,  0.0107,  0.0424, -0.0312,  0.0017, -0.0472,\n",
       "          0.0155,  0.0461]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first test of correctness to do if the architecture is defined properly\n",
    "x=X[:1].to(device)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m\n",
      "\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0min_features\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mout_features\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mbias\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "Applies an affine linear transformation to the incoming data: :math:`y = xA^T + b`.\n",
      "\n",
      "This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
      "\n",
      "On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
      "\n",
      "Args:\n",
      "    in_features: size of each input sample\n",
      "    out_features: size of each output sample\n",
      "    bias: If set to ``False``, the layer will not learn an additive bias.\n",
      "        Default: ``True``\n",
      "\n",
      "Shape:\n",
      "    - Input: :math:`(*, H_{in})` where :math:`*` means any number of\n",
      "      dimensions including none and :math:`H_{in} = \\text{in\\_features}`.\n",
      "    - Output: :math:`(*, H_{out})` where all but the last dimension\n",
      "      are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n",
      "\n",
      "Attributes:\n",
      "    weight: the learnable weights of the module of shape\n",
      "        :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
      "        initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
      "        :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      "    bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
      "            If :attr:`bias` is ``True``, the values are initialized from\n",
      "            :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
      "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      "\n",
      "Examples::\n",
      "\n",
      "    >>> m = nn.Linear(20, 30)\n",
      "    >>> input = torch.randn(128, 20)\n",
      "    >>> output = m(input)\n",
      "    >>> print(output.size())\n",
      "    torch.Size([128, 30])\n",
      "\u001b[1;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
      "\u001b[1;31mFile:\u001b[0m           c:\\users\\swarn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py\n",
      "\u001b[1;31mType:\u001b[0m           type\n",
      "\u001b[1;31mSubclasses:\u001b[0m     NonDynamicallyQuantizableLinear, LazyLinear, Linear, LinearBn1d, Linear"
     ]
    }
   ],
   "source": [
    "?nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 784])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([10, 512])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for layer in model.parameters():\n",
    "    print(layer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0109, -0.0009, -0.0196,  ..., -0.0088, -0.0117,  0.0234],\n",
       "        [ 0.0082, -0.0088, -0.0139,  ...,  0.0161,  0.0211,  0.0041],\n",
       "        [-0.0153,  0.0318,  0.0118,  ..., -0.0187,  0.0351,  0.0269],\n",
       "        ...,\n",
       "        [ 0.0257,  0.0046,  0.0030,  ...,  0.0094, -0.0333,  0.0286],\n",
       "        [ 0.0308,  0.0342, -0.0321,  ..., -0.0057, -0.0150, -0.0356],\n",
       "        [-0.0304, -0.0273,  0.0278,  ...,  0.0085, -0.0191, -0.0263]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for layer in model.parameters():\n",
    "    break\n",
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0109, -0.0009, -0.0196,  ..., -0.0088, -0.0117,  0.0234],\n",
       "         [ 0.0082, -0.0088, -0.0139,  ...,  0.0161,  0.0211,  0.0041],\n",
       "         [-0.0153,  0.0318,  0.0118,  ..., -0.0187,  0.0351,  0.0269],\n",
       "         ...,\n",
       "         [ 0.0257,  0.0046,  0.0030,  ...,  0.0094, -0.0333,  0.0286],\n",
       "         [ 0.0308,  0.0342, -0.0321,  ..., -0.0057, -0.0150, -0.0356],\n",
       "         [-0.0304, -0.0273,  0.0278,  ...,  0.0085, -0.0191, -0.0263]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 2.2789e-03, -5.6591e-03, -2.4196e-03, -2.7779e-03, -2.8576e-02,\n",
       "          1.7274e-02, -1.1701e-02, -1.9685e-02, -3.8120e-04,  3.3581e-04,\n",
       "         -3.0662e-02,  3.4025e-02,  2.3114e-02,  2.0034e-02, -1.9968e-02,\n",
       "         -2.4245e-02, -2.2184e-03, -2.1373e-03,  2.6402e-02, -3.4736e-02,\n",
       "          6.8038e-03,  2.1091e-02,  1.5972e-03,  2.4149e-02,  8.3124e-03,\n",
       "         -2.1750e-02,  1.0040e-02,  1.1564e-02,  2.5383e-02, -2.8041e-02,\n",
       "         -1.5489e-02, -2.1022e-02, -2.6855e-02,  2.0133e-02,  1.8174e-02,\n",
       "         -3.4085e-02, -1.8247e-02, -5.1166e-03,  7.7160e-03, -4.8208e-03,\n",
       "         -2.4215e-02, -3.5408e-03, -3.0688e-02, -3.4245e-02, -3.2998e-02,\n",
       "         -9.1976e-03,  9.3342e-03,  1.4684e-02, -1.3359e-02,  2.3001e-02,\n",
       "          2.5039e-02, -2.8752e-02, -1.9969e-02, -2.7774e-02,  3.1584e-02,\n",
       "         -3.0559e-02, -2.1452e-02, -2.4301e-03, -1.6816e-02,  1.8314e-02,\n",
       "         -1.9268e-02, -2.9472e-02, -2.0098e-02,  3.3795e-02,  5.0123e-03,\n",
       "          1.7317e-02, -1.6395e-02,  2.2864e-02,  2.1269e-02, -7.9434e-03,\n",
       "         -3.4423e-02,  7.0078e-03,  1.2899e-02, -5.5386e-03, -8.1618e-03,\n",
       "         -3.0178e-02, -8.8778e-03, -7.6381e-03, -9.4116e-03,  2.8572e-02,\n",
       "         -2.0090e-02,  2.3447e-02,  2.5042e-02, -5.1998e-03, -2.0415e-02,\n",
       "         -2.8087e-02, -8.7648e-03,  5.2364e-03, -3.8829e-03, -7.4897e-03,\n",
       "          1.8421e-02, -6.9900e-03,  1.6586e-02, -1.9697e-02,  1.7680e-02,\n",
       "         -1.5606e-02,  5.4639e-03, -2.6270e-02,  5.6334e-03, -2.4035e-02,\n",
       "          3.2756e-02, -8.9257e-03, -1.9034e-02, -2.1439e-02,  2.3043e-02,\n",
       "         -1.0036e-02, -2.6592e-02,  1.6015e-02, -2.7766e-02, -2.0701e-02,\n",
       "         -8.0504e-03, -2.6915e-02, -1.8705e-02, -9.4540e-03,  9.0669e-03,\n",
       "         -1.9157e-02, -2.1629e-02,  1.5245e-02,  9.6579e-03, -5.3662e-04,\n",
       "         -4.1487e-03,  3.3879e-02,  3.1725e-02,  2.2904e-02, -2.5437e-02,\n",
       "         -3.2598e-02,  1.1836e-02,  1.2798e-02, -3.3322e-02,  9.9920e-05,\n",
       "          2.3059e-02,  1.4292e-02,  3.5233e-02, -2.9136e-03,  2.4689e-02,\n",
       "         -1.5017e-02,  2.3234e-02,  1.1341e-02,  1.7970e-02,  3.3898e-02,\n",
       "          3.5214e-02, -3.3291e-02,  5.8676e-03, -1.1075e-02, -1.3467e-02,\n",
       "          1.8334e-02, -1.7628e-02, -3.5371e-02, -4.8680e-03, -3.0134e-02,\n",
       "         -3.2556e-02,  3.1619e-02, -3.1262e-02, -1.5192e-02, -9.4663e-03,\n",
       "          1.7500e-02, -4.0192e-03, -3.5659e-02, -1.6828e-02,  6.8571e-03,\n",
       "         -4.3100e-03, -1.0886e-03,  3.3538e-02,  2.7869e-02, -1.8967e-02,\n",
       "          3.1805e-02, -2.1422e-02,  2.7001e-02, -5.8238e-03, -2.5851e-02,\n",
       "         -3.3186e-02, -1.8098e-02,  2.4257e-02, -2.4178e-02,  2.4665e-02,\n",
       "          1.4079e-02, -5.0414e-04, -3.3195e-02, -2.7116e-02, -2.2186e-02,\n",
       "          1.2938e-02,  2.1021e-02,  5.3459e-03, -1.0198e-02, -2.2746e-02,\n",
       "          1.8649e-02, -7.8267e-03, -1.5204e-02,  7.9475e-05,  1.5338e-03,\n",
       "          6.7566e-03,  8.6961e-03, -7.0314e-03, -1.0627e-02, -2.1489e-02,\n",
       "         -1.5609e-02, -2.4978e-02,  2.2790e-02,  1.2084e-02, -2.8551e-03,\n",
       "          3.1912e-02, -2.3656e-02,  6.8161e-03,  1.1632e-02, -1.5258e-02,\n",
       "         -3.0186e-02,  2.9987e-02,  3.4011e-02,  1.1593e-03, -7.3830e-03,\n",
       "         -3.0473e-02, -1.2953e-02,  2.5551e-02, -1.6102e-02, -2.4001e-03,\n",
       "         -2.4589e-02, -2.0268e-02, -2.2503e-02, -2.0294e-02, -1.6419e-03,\n",
       "          2.9177e-02, -1.4202e-02, -1.1838e-02,  8.8008e-03, -2.1033e-02,\n",
       "         -1.0243e-02, -2.8860e-02,  1.5945e-02,  1.3390e-02,  3.4338e-02,\n",
       "         -2.4711e-02,  1.4642e-02,  9.7681e-03,  2.9232e-02,  6.9424e-03,\n",
       "          2.8516e-02, -1.7070e-02,  2.9835e-03, -2.6682e-02,  2.2647e-02,\n",
       "          1.2741e-02,  2.4804e-02, -5.0893e-03,  3.4040e-02,  1.5709e-02,\n",
       "          3.3689e-02,  6.0632e-03, -4.3379e-03, -7.0354e-03, -6.1161e-03,\n",
       "          1.9981e-02,  3.4820e-02,  1.1609e-02,  2.3682e-02, -9.7119e-03,\n",
       "         -1.9612e-02, -2.4657e-02,  7.0025e-03,  2.8607e-02,  1.9722e-03,\n",
       "         -3.2908e-02,  5.4508e-03, -2.5859e-02, -9.9871e-03, -7.1857e-03,\n",
       "         -1.2051e-03, -6.4000e-03,  1.2698e-02,  3.2664e-02,  2.6631e-02,\n",
       "         -7.9151e-03,  2.0004e-02, -1.7715e-02,  8.5013e-03,  1.3196e-02,\n",
       "          3.2370e-02, -1.3279e-02, -5.6907e-03, -2.5939e-02,  1.7877e-02,\n",
       "         -2.8626e-02, -3.0028e-02,  2.4074e-02, -1.3241e-02, -2.6391e-02,\n",
       "          2.8046e-02, -8.3355e-03,  3.3039e-02, -1.3133e-02, -3.5559e-03,\n",
       "          1.3515e-02, -1.8944e-02,  2.7324e-02,  2.8141e-02, -5.4484e-03,\n",
       "          3.4890e-02, -5.3149e-03,  2.6612e-02, -8.4627e-03,  2.2117e-03,\n",
       "          9.9921e-03, -1.4257e-02, -3.2287e-02, -1.0300e-02,  2.9723e-02,\n",
       "          2.5985e-02,  2.5284e-02,  1.1935e-03,  5.0563e-03, -3.0536e-02,\n",
       "          2.7714e-02, -2.3602e-02, -1.2964e-02, -2.9063e-02,  2.6089e-03,\n",
       "         -1.2730e-02,  2.4508e-02,  1.1531e-02,  2.9936e-02, -1.6035e-02,\n",
       "          2.5561e-02,  5.2344e-03,  2.3326e-02,  3.5394e-02,  2.6809e-02,\n",
       "          8.7248e-03, -2.9851e-03,  3.5365e-02, -7.0369e-03,  2.8533e-02,\n",
       "         -1.2260e-02, -1.6210e-02, -5.5005e-03, -7.3633e-03, -1.0659e-02,\n",
       "         -4.5039e-04, -1.1276e-02, -5.5061e-03,  2.9116e-02, -1.9185e-02,\n",
       "          2.1580e-02, -3.5373e-02, -7.8066e-03, -6.6480e-03, -3.1847e-02,\n",
       "         -2.6466e-02,  1.2507e-02, -2.8130e-03,  2.8766e-02,  6.2356e-03,\n",
       "         -7.9606e-04,  1.8510e-02,  2.7206e-02, -3.1779e-02,  2.3581e-02,\n",
       "          8.1161e-03, -2.7505e-02,  8.2116e-04, -2.3848e-02,  1.3844e-02,\n",
       "          3.0690e-02, -1.5887e-02, -9.7034e-03, -7.0295e-04, -1.9813e-02,\n",
       "         -1.3427e-02, -2.1546e-02,  2.7439e-02,  2.2297e-02,  2.8536e-02,\n",
       "          1.6402e-02,  2.7162e-02, -2.3873e-02,  7.1409e-03, -1.1470e-02,\n",
       "         -1.0817e-02, -1.2788e-02, -1.3088e-03,  2.7672e-02,  3.3335e-03,\n",
       "          3.4074e-02, -1.5504e-02, -2.5190e-02,  2.3496e-02,  1.3816e-02,\n",
       "          1.2319e-02,  1.3202e-02,  1.5025e-02, -1.3636e-02,  6.4885e-03,\n",
       "         -5.2153e-03, -2.4432e-02, -1.9171e-03,  1.7805e-02,  5.6680e-03,\n",
       "          2.3831e-03, -1.4623e-02, -2.3578e-02, -2.5438e-02,  1.9490e-02,\n",
       "         -6.7619e-03, -1.3942e-02,  3.3505e-02, -6.1279e-03,  2.6022e-02,\n",
       "         -2.5579e-02,  2.9579e-06, -5.7965e-03,  3.3372e-02, -2.8847e-02,\n",
       "         -3.3563e-03,  2.6298e-02, -1.0949e-02,  3.5174e-02, -2.3443e-02,\n",
       "         -1.6412e-02, -1.5722e-02, -1.4694e-03, -1.6639e-02, -1.4343e-02,\n",
       "         -2.7670e-02, -1.9129e-02,  1.5450e-02,  1.2162e-02,  2.6524e-02,\n",
       "          1.8949e-02,  4.5330e-03, -2.3974e-02, -2.7113e-02, -7.2599e-03,\n",
       "         -2.3899e-02,  2.0696e-02, -1.4603e-02, -3.0564e-02, -2.4951e-02,\n",
       "          1.1554e-02, -2.9798e-03, -1.2076e-02,  1.3169e-02, -1.1373e-03,\n",
       "          1.6033e-02, -5.9155e-03,  4.9223e-03,  3.5264e-02, -3.2432e-02,\n",
       "         -2.3131e-02, -4.7775e-03, -2.2892e-02, -5.6460e-03,  1.0663e-02,\n",
       "          3.2502e-02,  7.4144e-03, -1.3580e-02, -3.2642e-02, -2.6608e-02,\n",
       "         -8.1494e-03, -1.9178e-02,  2.8316e-02,  3.3974e-02, -5.5729e-03,\n",
       "         -1.8569e-02, -3.0262e-02,  6.4049e-03,  1.0767e-02,  1.3982e-02,\n",
       "          1.0983e-03,  2.7765e-02,  3.1765e-02, -2.0991e-02,  5.7574e-03,\n",
       "          2.1729e-02, -1.1830e-02, -2.9375e-02,  1.0051e-02, -3.0156e-02,\n",
       "         -1.0910e-02,  1.1146e-02,  3.2378e-03, -1.2928e-03, -1.0825e-02,\n",
       "         -1.2780e-04,  2.1105e-02,  2.6694e-02, -2.4827e-02,  1.3375e-02,\n",
       "          1.7597e-03, -2.7305e-02, -1.4962e-02, -2.5638e-02, -3.4481e-02,\n",
       "         -2.6759e-02,  1.9247e-02,  2.5382e-02, -1.6133e-02,  3.4676e-02,\n",
       "          2.1511e-02,  3.3083e-02, -4.4276e-03,  3.2251e-02, -2.4301e-02,\n",
       "          3.9404e-03, -1.0676e-02,  1.3729e-02,  3.3316e-02,  3.3434e-02,\n",
       "         -2.2872e-02, -4.9858e-03, -3.0446e-02, -3.3643e-04,  5.0046e-03,\n",
       "         -6.5660e-03, -7.2082e-03], requires_grad=True)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.linear_relu_stack[0].parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=nn.CrossEntropyLoss()\n",
    "optimizer= torch.optim.SGD(model.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0085,  0.0344, -0.0747,  0.0107,  0.0424, -0.0312,  0.0017, -0.0472,\n",
      "          0.0155,  0.0461]], grad_fn=<AddmmBackward0>) torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "pred=model(x)\n",
    "print(pred, pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0085,  0.0344, -0.0747,  0.0107,  0.0424, -0.0312,  0.0017, -0.0472,\n",
      "          0.0155,  0.0461]], grad_fn=<AddmmBackward0>) torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "x=X[:1].to(device)\n",
    "pred = model(x)\n",
    "print(pred, pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3511, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(model(x), y[:1].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1007, 0.1034, 0.0927, 0.1009, 0.1042, 0.0968, 0.1000, 0.0953, 0.1014,\n",
       "         0.1046]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Softmax(dim=1)(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0953, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Softmax(dim=1)(pred)[0][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3507254683219805"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import log\n",
    "-log(0.0953)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size=len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # compute prediction error\n",
    "        pred = model(X)\n",
    "        loss=loss_fn(pred,y)\n",
    "\n",
    "        #Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch+1) * len(X)\n",
    "            print(f\"loss:{loss:>7f} [{current:>5d} / {size:>5d}]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size=len(dataloader.dataset)\n",
    "    num_batches=len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss,correct=0,0\n",
    "    with torch.no_grad():\n",
    "        for X,y in dataloader:\n",
    "            X, y=X.to(device), y.to(device)\n",
    "            pred=model(X)\n",
    "            test_loss+=loss_fn(pred,y).item()\n",
    "            correct+= (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        test_loss /= num_batches\n",
    "        correct /= size\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg Loss:{test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0217,  0.0750, -0.0456, -0.0033, -0.0627, -0.0790, -0.0055, -0.0738,\n",
       "          0.1414,  0.0311],\n",
       "        [-0.0296,  0.0780, -0.0488, -0.0125, -0.0565, -0.0733, -0.0057, -0.0774,\n",
       "          0.1448,  0.0343],\n",
       "        [-0.0272,  0.0750, -0.0477, -0.0109, -0.0568, -0.0754, -0.0109, -0.0733,\n",
       "          0.1424,  0.0341],\n",
       "        [-0.0213,  0.0743, -0.0459, -0.0079, -0.0599, -0.0780, -0.0002, -0.0736,\n",
       "          0.1403,  0.0323],\n",
       "        [-0.0207,  0.0789, -0.0480, -0.0055, -0.0617, -0.0767, -0.0090, -0.0710,\n",
       "          0.1391,  0.0320],\n",
       "        [-0.0258,  0.0763, -0.0488, -0.0134, -0.0586, -0.0730, -0.0085, -0.0735,\n",
       "          0.1429,  0.0319],\n",
       "        [-0.0190,  0.0773, -0.0485, -0.0030, -0.0617, -0.0794, -0.0060, -0.0731,\n",
       "          0.1396,  0.0271],\n",
       "        [-0.0219,  0.0755, -0.0506, -0.0085, -0.0599, -0.0765, -0.0071, -0.0749,\n",
       "          0.1431,  0.0287],\n",
       "        [-0.0254,  0.0745, -0.0479, -0.0127, -0.0588, -0.0767, -0.0040, -0.0700,\n",
       "          0.1390,  0.0337],\n",
       "        [-0.0219,  0.0763, -0.0485, -0.0089, -0.0585, -0.0737, -0.0068, -0.0749,\n",
       "          0.1411,  0.0273],\n",
       "        [-0.0219,  0.0755, -0.0467, -0.0058, -0.0601, -0.0751, -0.0043, -0.0721,\n",
       "          0.1424,  0.0321],\n",
       "        [-0.0245,  0.0773, -0.0458, -0.0028, -0.0577, -0.0723, -0.0097, -0.0779,\n",
       "          0.1455,  0.0333],\n",
       "        [-0.0199,  0.0772, -0.0483, -0.0121, -0.0592, -0.0790, -0.0062, -0.0733,\n",
       "          0.1376,  0.0282],\n",
       "        [-0.0235,  0.0751, -0.0465, -0.0056, -0.0570, -0.0782, -0.0047, -0.0714,\n",
       "          0.1418,  0.0336],\n",
       "        [-0.0294,  0.0736, -0.0508, -0.0118, -0.0552, -0.0763, -0.0058, -0.0753,\n",
       "          0.1420,  0.0342],\n",
       "        [-0.0227,  0.0745, -0.0461, -0.0043, -0.0577, -0.0809, -0.0048, -0.0730,\n",
       "          0.1409,  0.0333],\n",
       "        [-0.0221,  0.0770, -0.0476, -0.0068, -0.0615, -0.0781, -0.0085, -0.0730,\n",
       "          0.1427,  0.0311],\n",
       "        [-0.0237,  0.0754, -0.0454, -0.0040, -0.0626, -0.0775, -0.0071, -0.0740,\n",
       "          0.1440,  0.0341],\n",
       "        [-0.0223,  0.0821, -0.0488, -0.0097, -0.0599, -0.0728, -0.0065, -0.0755,\n",
       "          0.1445,  0.0344],\n",
       "        [-0.0195,  0.0772, -0.0475, -0.0052, -0.0604, -0.0792, -0.0065, -0.0724,\n",
       "          0.1386,  0.0294],\n",
       "        [-0.0227,  0.0764, -0.0523, -0.0093, -0.0583, -0.0749, -0.0047, -0.0743,\n",
       "          0.1388,  0.0286],\n",
       "        [-0.0246,  0.0788, -0.0459, -0.0115, -0.0584, -0.0753, -0.0089, -0.0753,\n",
       "          0.1435,  0.0338],\n",
       "        [-0.0218,  0.0761, -0.0485, -0.0060, -0.0623, -0.0711, -0.0101, -0.0711,\n",
       "          0.1410,  0.0355],\n",
       "        [-0.0259,  0.0706, -0.0452, -0.0060, -0.0565, -0.0815, -0.0023, -0.0764,\n",
       "          0.1437,  0.0308],\n",
       "        [-0.0236,  0.0744, -0.0478, -0.0108, -0.0599, -0.0792, -0.0104, -0.0725,\n",
       "          0.1430,  0.0308],\n",
       "        [-0.0213,  0.0816, -0.0464, -0.0071, -0.0605, -0.0688,  0.0007, -0.0756,\n",
       "          0.1434,  0.0321],\n",
       "        [-0.0280,  0.0737, -0.0468, -0.0060, -0.0626, -0.0773, -0.0073, -0.0734,\n",
       "          0.1455,  0.0362],\n",
       "        [-0.0209,  0.0748, -0.0490, -0.0065, -0.0596, -0.0820, -0.0071, -0.0698,\n",
       "          0.1378,  0.0309],\n",
       "        [-0.0215,  0.0771, -0.0483, -0.0078, -0.0585, -0.0744, -0.0007, -0.0756,\n",
       "          0.1409,  0.0283],\n",
       "        [-0.0265,  0.0706, -0.0491, -0.0124, -0.0557, -0.0817, -0.0049, -0.0742,\n",
       "          0.1402,  0.0286],\n",
       "        [-0.0241,  0.0758, -0.0452, -0.0090, -0.0591, -0.0730, -0.0047, -0.0777,\n",
       "          0.1440,  0.0351],\n",
       "        [-0.0254,  0.0722, -0.0493, -0.0096, -0.0598, -0.0794, -0.0070, -0.0722,\n",
       "          0.1399,  0.0284],\n",
       "        [-0.0243,  0.0791, -0.0454, -0.0111, -0.0568, -0.0709, -0.0055, -0.0807,\n",
       "          0.1453,  0.0294],\n",
       "        [-0.0220,  0.0817, -0.0472, -0.0072, -0.0579, -0.0771, -0.0055, -0.0722,\n",
       "          0.1410,  0.0345],\n",
       "        [-0.0234,  0.0756, -0.0504, -0.0050, -0.0623, -0.0778, -0.0031, -0.0726,\n",
       "          0.1424,  0.0290],\n",
       "        [-0.0245,  0.0785, -0.0457, -0.0048, -0.0618, -0.0783, -0.0070, -0.0721,\n",
       "          0.1477,  0.0330],\n",
       "        [-0.0217,  0.0760, -0.0485, -0.0029, -0.0651, -0.0772, -0.0052, -0.0704,\n",
       "          0.1410,  0.0328],\n",
       "        [-0.0286,  0.0711, -0.0504, -0.0099, -0.0543, -0.0792, -0.0052, -0.0744,\n",
       "          0.1403,  0.0324],\n",
       "        [-0.0257,  0.0768, -0.0489, -0.0098, -0.0595, -0.0785, -0.0051, -0.0727,\n",
       "          0.1425,  0.0346],\n",
       "        [-0.0277,  0.0744, -0.0496, -0.0131, -0.0552, -0.0752, -0.0071, -0.0755,\n",
       "          0.1424,  0.0320],\n",
       "        [-0.0296,  0.0728, -0.0467, -0.0127, -0.0546, -0.0804, -0.0080, -0.0746,\n",
       "          0.1425,  0.0337],\n",
       "        [-0.0231,  0.0741, -0.0458, -0.0053, -0.0618, -0.0795, -0.0069, -0.0726,\n",
       "          0.1446,  0.0330],\n",
       "        [-0.0224,  0.0796, -0.0482, -0.0115, -0.0576, -0.0725, -0.0101, -0.0731,\n",
       "          0.1391,  0.0311],\n",
       "        [-0.0305,  0.0760, -0.0502, -0.0093, -0.0572, -0.0807, -0.0058, -0.0719,\n",
       "          0.1438,  0.0377],\n",
       "        [-0.0250,  0.0729, -0.0509, -0.0096, -0.0566, -0.0787, -0.0049, -0.0762,\n",
       "          0.1415,  0.0284],\n",
       "        [-0.0240,  0.0752, -0.0500, -0.0079, -0.0595, -0.0761, -0.0068, -0.0737,\n",
       "          0.1405,  0.0296],\n",
       "        [-0.0284,  0.0746, -0.0492, -0.0075, -0.0581, -0.0740, -0.0095, -0.0757,\n",
       "          0.1441,  0.0324],\n",
       "        [-0.0262,  0.0768, -0.0474, -0.0087, -0.0564, -0.0794, -0.0069, -0.0728,\n",
       "          0.1438,  0.0332],\n",
       "        [-0.0209,  0.0797, -0.0510, -0.0080, -0.0575, -0.0710, -0.0067, -0.0710,\n",
       "          0.1364,  0.0304],\n",
       "        [-0.0230,  0.0810, -0.0484, -0.0085, -0.0585, -0.0713, -0.0076, -0.0756,\n",
       "          0.1428,  0.0333],\n",
       "        [-0.0243,  0.0780, -0.0439, -0.0125, -0.0586, -0.0739, -0.0049, -0.0754,\n",
       "          0.1441,  0.0348],\n",
       "        [-0.0199,  0.0779, -0.0458, -0.0128, -0.0538, -0.0772, -0.0032, -0.0741,\n",
       "          0.1407,  0.0334],\n",
       "        [-0.0197,  0.0748, -0.0465, -0.0086, -0.0597, -0.0815, -0.0062, -0.0706,\n",
       "          0.1378,  0.0310],\n",
       "        [-0.0217,  0.0776, -0.0503, -0.0048, -0.0633, -0.0805, -0.0026, -0.0728,\n",
       "          0.1443,  0.0308],\n",
       "        [-0.0300,  0.0780, -0.0437, -0.0087, -0.0571, -0.0671, -0.0102, -0.0789,\n",
       "          0.1472,  0.0372],\n",
       "        [-0.0279,  0.0740, -0.0477, -0.0090, -0.0546, -0.0775, -0.0034, -0.0774,\n",
       "          0.1431,  0.0340],\n",
       "        [-0.0186,  0.0840, -0.0488, -0.0077, -0.0587, -0.0721, -0.0053, -0.0728,\n",
       "          0.1391,  0.0303],\n",
       "        [-0.0296,  0.0759, -0.0498, -0.0108, -0.0580, -0.0774, -0.0114, -0.0724,\n",
       "          0.1408,  0.0368],\n",
       "        [-0.0210,  0.0780, -0.0487, -0.0090, -0.0596, -0.0768, -0.0092, -0.0702,\n",
       "          0.1372,  0.0332],\n",
       "        [-0.0271,  0.0743, -0.0494, -0.0078, -0.0605, -0.0790, -0.0093, -0.0734,\n",
       "          0.1408,  0.0341],\n",
       "        [-0.0272,  0.0761, -0.0494, -0.0028, -0.0576, -0.0738, -0.0085, -0.0765,\n",
       "          0.1456,  0.0365],\n",
       "        [-0.0284,  0.0749, -0.0475, -0.0068, -0.0584, -0.0758, -0.0025, -0.0763,\n",
       "          0.1498,  0.0322],\n",
       "        [-0.0223,  0.0738, -0.0495, -0.0030, -0.0630, -0.0801, -0.0023, -0.0727,\n",
       "          0.1451,  0.0319],\n",
       "        [-0.0268,  0.0758, -0.0472, -0.0089, -0.0580, -0.0778, -0.0020, -0.0778,\n",
       "          0.1473,  0.0295]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n",
    "        # input: 28 * 28\n",
    "        # after first conv with kernel size 5: 24 * 24\n",
    "        # after first pool: 12 * 12\n",
    "        # after second conv with kernel size 5: 8 * 8\n",
    "        # after second pool: 4 * 4\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "X = X.to(device)\n",
    "x = X[0].to(device)\n",
    "model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 11.3%, Avg Loss:2.299606 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss:2.296834 [   64 / 60000]\n",
      "loss:2.300938 [ 6464 / 60000]\n",
      "loss:2.306524 [12864 / 60000]\n",
      "loss:2.298632 [19264 / 60000]\n",
      "loss:2.307951 [25664 / 60000]\n",
      "loss:2.302047 [32064 / 60000]\n",
      "loss:2.289590 [38464 / 60000]\n",
      "loss:2.308387 [44864 / 60000]\n",
      "loss:2.301643 [51264 / 60000]\n",
      "loss:2.293241 [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 11.3%, Avg Loss:2.298927 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss:2.296221 [   64 / 60000]\n",
      "loss:2.300029 [ 6464 / 60000]\n",
      "loss:2.305133 [12864 / 60000]\n",
      "loss:2.297882 [19264 / 60000]\n",
      "loss:2.307358 [25664 / 60000]\n",
      "loss:2.301838 [32064 / 60000]\n",
      "loss:2.289240 [38464 / 60000]\n",
      "loss:2.307648 [44864 / 60000]\n",
      "loss:2.301424 [51264 / 60000]\n",
      "loss:2.292326 [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 11.3%, Avg Loss:2.298194 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss:2.295573 [   64 / 60000]\n",
      "loss:2.299100 [ 6464 / 60000]\n",
      "loss:2.303770 [12864 / 60000]\n",
      "loss:2.297034 [19264 / 60000]\n",
      "loss:2.306665 [25664 / 60000]\n",
      "loss:2.301498 [32064 / 60000]\n",
      "loss:2.288760 [38464 / 60000]\n",
      "loss:2.306866 [44864 / 60000]\n",
      "loss:2.301094 [51264 / 60000]\n",
      "loss:2.291325 [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 11.3%, Avg Loss:2.297373 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss:2.294831 [   64 / 60000]\n",
      "loss:2.298124 [ 6464 / 60000]\n",
      "loss:2.302396 [12864 / 60000]\n",
      "loss:2.296060 [19264 / 60000]\n",
      "loss:2.305840 [25664 / 60000]\n",
      "loss:2.301001 [32064 / 60000]\n",
      "loss:2.288113 [38464 / 60000]\n",
      "loss:2.305973 [44864 / 60000]\n",
      "loss:2.300593 [51264 / 60000]\n",
      "loss:2.290216 [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 11.3%, Avg Loss:2.296422 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss:2.293944 [   64 / 60000]\n",
      "loss:2.297052 [ 6464 / 60000]\n",
      "loss:2.300960 [12864 / 60000]\n",
      "loss:2.294918 [19264 / 60000]\n",
      "loss:2.304835 [25664 / 60000]\n",
      "loss:2.300312 [32064 / 60000]\n",
      "loss:2.287227 [38464 / 60000]\n",
      "loss:2.304903 [44864 / 60000]\n",
      "loss:2.299850 [51264 / 60000]\n",
      "loss:2.288913 [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 11.3%, Avg Loss:2.295278 \n",
      "\n",
      "Done in  168.74292659759521  seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "epochs = 5\n",
    "test(test_dataloader, model, loss_fn)\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done in \", time.time() - start, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 9.7%, Avg Loss:2.303660 \n",
      "\n",
      "Epoch1\n",
      " -----------------\n",
      "loss:2.300646 [   64 / 60000]\n",
      "loss:2.306633 [ 6464 / 60000]\n",
      "loss:2.315456 [12864 / 60000]\n",
      "loss:2.301937 [19264 / 60000]\n",
      "loss:2.310923 [25664 / 60000]\n",
      "loss:2.301942 [32064 / 60000]\n",
      "loss:2.290546 [38464 / 60000]\n",
      "loss:2.312359 [44864 / 60000]\n",
      "loss:2.301614 [51264 / 60000]\n",
      "loss:2.298311 [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 9.7%, Avg Loss:2.302588 \n",
      "\n",
      "Epoch2\n",
      " -----------------\n",
      "loss:2.299626 [   64 / 60000]\n",
      "loss:2.305110 [ 6464 / 60000]\n",
      "loss:2.313161 [12864 / 60000]\n",
      "loss:2.301235 [19264 / 60000]\n",
      "loss:2.310197 [25664 / 60000]\n",
      "loss:2.301986 [32064 / 60000]\n",
      "loss:2.290320 [38464 / 60000]\n",
      "loss:2.311318 [44864 / 60000]\n",
      "loss:2.301658 [51264 / 60000]\n",
      "loss:2.297103 [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 9.7%, Avg Loss:2.301700 \n",
      "\n",
      "Epoch3\n",
      " -----------------\n",
      "loss:2.298800 [   64 / 60000]\n",
      "loss:2.303856 [ 6464 / 60000]\n",
      "loss:2.311201 [12864 / 60000]\n",
      "loss:2.300606 [19264 / 60000]\n",
      "loss:2.309592 [25664 / 60000]\n",
      "loss:2.302070 [32064 / 60000]\n",
      "loss:2.290156 [38464 / 60000]\n",
      "loss:2.310479 [44864 / 60000]\n",
      "loss:2.301713 [51264 / 60000]\n",
      "loss:2.296017 [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 17.6%, Avg Loss:2.300945 \n",
      "\n",
      "Epoch4\n",
      " -----------------\n",
      "loss:2.298083 [   64 / 60000]\n",
      "loss:2.302794 [ 6464 / 60000]\n",
      "loss:2.309502 [12864 / 60000]\n",
      "loss:2.299974 [19264 / 60000]\n",
      "loss:2.309042 [25664 / 60000]\n",
      "loss:2.302142 [32064 / 60000]\n",
      "loss:2.290026 [38464 / 60000]\n",
      "loss:2.309758 [44864 / 60000]\n",
      "loss:2.301754 [51264 / 60000]\n",
      "loss:2.295028 [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 11.4%, Avg Loss:2.300265 \n",
      "\n",
      "Epoch5\n",
      " -----------------\n",
      "loss:2.297446 [   64 / 60000]\n",
      "loss:2.301849 [ 6464 / 60000]\n",
      "loss:2.307964 [12864 / 60000]\n",
      "loss:2.299321 [19264 / 60000]\n",
      "loss:2.308503 [25664 / 60000]\n",
      "loss:2.302143 [32064 / 60000]\n",
      "loss:2.289850 [38464 / 60000]\n",
      "loss:2.309072 [44864 / 60000]\n",
      "loss:2.301742 [51264 / 60000]\n",
      "loss:2.294116 [57664 / 60000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[127], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m -----------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m     train(train_dataloader, model, loss_fn, optimizer)\n\u001b[1;32m----> 8\u001b[0m     \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone in \u001b[39m\u001b[38;5;124m\"\u001b[39m,time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mstart,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[126], line 7\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(dataloader, model, loss_fn)\u001b[0m\n\u001b[0;32m      5\u001b[0m test_loss,correct\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X,y \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m      8\u001b[0m         X, y\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m         pred\u001b[38;5;241m=\u001b[39mmodel(X)\n",
      "File \u001b[1;32mc:\\Users\\swarn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\swarn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\swarn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\swarn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\swarn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\mnist.py:146\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    143\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\swarn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\swarn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:168\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[0;32m    167\u001b[0m mode_to_nptype \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint32, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mbyteorder \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlittle\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16B\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint16, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mfloat32}\n\u001b[1;32m--> 168\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_to_nptype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    171\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "epochs = 5\n",
    "test(test_dataloader, model, loss_fn)\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch{t+1}\\n -----------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"done in \",time.time()-start,\"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2103, -0.5768, -0.5049, -0.1126,  0.2200, -0.2347, -0.5082,  1.2533,\n",
      "         -0.1401,  0.6384]], grad_fn=<AddmmBackward0>) torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "preds = model(X[0].to(device))\n",
    "print(preds, preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0688, 0.0477, 0.0513, 0.0759, 0.1058, 0.0672, 0.0511, 0.2975, 0.0738,\n",
       "         0.1608]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Softmax(dim=1)(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2124, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(preds, y[:1].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-9.9841e-02, -3.8727e-02, -6.4952e-02,  2.1847e-02, -6.4567e-02,\n",
       "         -9.3095e-03, -4.5634e-02, -5.5343e-02, -1.3707e-01,  6.0026e-02],\n",
       "        [-9.7059e-02, -4.4414e-02, -5.9556e-02,  2.4742e-02, -5.2004e-02,\n",
       "         -9.2807e-03, -5.0665e-02, -5.6921e-02, -1.2367e-01,  7.3926e-02],\n",
       "        [-1.0271e-01, -2.9861e-02, -5.8495e-02,  1.8603e-02, -6.0428e-02,\n",
       "         -1.4645e-02, -4.5898e-02, -6.2493e-02, -1.2155e-01,  5.7803e-02],\n",
       "        [-8.6071e-02, -4.5001e-02, -6.1220e-02,  2.6157e-02, -5.0043e-02,\n",
       "         -9.5405e-03, -3.0042e-02, -4.8531e-02, -1.3846e-01,  7.5483e-02],\n",
       "        [-1.0281e-01, -3.8131e-02, -6.8792e-02,  3.5083e-02, -5.5215e-02,\n",
       "         -4.7018e-03, -4.7627e-02, -5.6417e-02, -1.4586e-01,  6.9175e-02],\n",
       "        [-1.0438e-01, -3.0401e-02, -5.9449e-02,  1.7384e-02, -5.7207e-02,\n",
       "         -1.6640e-02, -4.5630e-02, -6.2067e-02, -1.2147e-01,  6.2477e-02],\n",
       "        [-1.0056e-01, -4.3178e-02, -7.0216e-02,  2.2878e-02, -6.2116e-02,\n",
       "         -1.4168e-02, -3.9688e-02, -5.2443e-02, -1.3568e-01,  6.1952e-02],\n",
       "        [-8.8075e-02, -4.7592e-02, -7.1248e-02,  3.7518e-02, -6.0336e-02,\n",
       "          1.3931e-04, -4.5794e-02, -4.9208e-02, -1.4602e-01,  7.2955e-02],\n",
       "        [-1.1180e-01, -3.6435e-02, -6.4220e-02,  4.4871e-02, -4.5670e-02,\n",
       "          1.1977e-03, -5.4572e-02, -6.1021e-02, -1.4996e-01,  7.4692e-02],\n",
       "        [-9.0398e-02, -4.8227e-02, -6.9972e-02,  2.7007e-02, -4.8654e-02,\n",
       "         -7.8989e-03, -2.6016e-02, -4.4939e-02, -1.4018e-01,  6.8545e-02],\n",
       "        [-9.8139e-02, -3.6610e-02, -6.8492e-02,  3.1004e-02, -4.8451e-02,\n",
       "         -3.1680e-03, -4.8151e-02, -4.9942e-02, -1.2981e-01,  7.3977e-02],\n",
       "        [-9.4846e-02, -5.3816e-02, -6.6350e-02,  2.8331e-02, -5.0673e-02,\n",
       "         -6.2389e-03, -4.8560e-02, -5.7826e-02, -1.3356e-01,  7.2733e-02],\n",
       "        [-9.8000e-02, -3.9224e-02, -6.5546e-02,  3.1238e-02, -5.3005e-02,\n",
       "         -1.5526e-02, -3.0392e-02, -5.0908e-02, -1.3055e-01,  6.0732e-02],\n",
       "        [-9.3507e-02, -3.8365e-02, -5.9755e-02,  2.9103e-02, -4.6587e-02,\n",
       "         -3.6397e-03, -4.2070e-02, -4.6850e-02, -1.3738e-01,  7.8172e-02],\n",
       "        [-9.3366e-02, -4.4065e-02, -5.9750e-02,  1.9920e-02, -6.4428e-02,\n",
       "         -1.3840e-02, -4.1614e-02, -5.6873e-02, -1.2463e-01,  7.0576e-02],\n",
       "        [-8.7978e-02, -5.2317e-02, -6.7606e-02,  2.2481e-02, -6.2765e-02,\n",
       "         -8.8219e-03, -3.9905e-02, -4.5575e-02, -1.3509e-01,  7.4842e-02],\n",
       "        [-9.9580e-02, -4.0307e-02, -7.0314e-02,  3.3736e-02, -5.4248e-02,\n",
       "         -7.6687e-03, -4.3703e-02, -5.4244e-02, -1.3886e-01,  6.4377e-02],\n",
       "        [-9.5279e-02, -4.2950e-02, -6.6335e-02,  1.6233e-02, -6.9124e-02,\n",
       "         -1.3395e-02, -3.7549e-02, -5.1895e-02, -1.4190e-01,  6.4869e-02],\n",
       "        [-8.9015e-02, -6.1762e-02, -6.9845e-02,  2.5195e-02, -6.2120e-02,\n",
       "         -7.0557e-04, -4.3348e-02, -4.2860e-02, -1.4904e-01,  7.5263e-02],\n",
       "        [-9.4607e-02, -3.9143e-02, -6.7464e-02,  2.9507e-02, -5.4727e-02,\n",
       "         -9.2971e-03, -3.6443e-02, -5.3041e-02, -1.2893e-01,  6.2108e-02],\n",
       "        [-9.6314e-02, -4.3985e-02, -6.5639e-02,  1.4492e-02, -5.8134e-02,\n",
       "         -1.9960e-02, -2.4082e-02, -5.8091e-02, -1.3025e-01,  6.4013e-02],\n",
       "        [-9.2888e-02, -5.1404e-02, -6.1794e-02,  3.4031e-02, -5.1078e-02,\n",
       "          2.0446e-03, -4.1723e-02, -4.6008e-02, -1.3896e-01,  7.2889e-02],\n",
       "        [-9.7766e-02, -4.2977e-02, -5.5775e-02,  3.3872e-02, -5.8167e-02,\n",
       "         -2.5028e-03, -5.1110e-02, -5.4056e-02, -1.4242e-01,  6.9499e-02],\n",
       "        [-9.4770e-02, -3.6631e-02, -7.3919e-02,  3.0846e-02, -5.9605e-02,\n",
       "         -1.2249e-02, -3.5330e-02, -5.1870e-02, -1.3237e-01,  6.6375e-02],\n",
       "        [-1.0134e-01, -3.4795e-02, -7.0403e-02,  2.9957e-02, -6.2009e-02,\n",
       "         -1.3099e-02, -3.8221e-02, -5.7884e-02, -1.3664e-01,  6.4684e-02],\n",
       "        [-8.6659e-02, -4.4568e-02, -6.7239e-02,  3.3136e-02, -4.5793e-02,\n",
       "          1.4378e-04, -2.9871e-02, -4.1039e-02, -1.4355e-01,  8.6486e-02],\n",
       "        [-9.2094e-02, -4.0171e-02, -6.5539e-02,  1.8506e-02, -6.8648e-02,\n",
       "         -1.3520e-02, -3.8532e-02, -5.3855e-02, -1.3733e-01,  6.2523e-02],\n",
       "        [-9.7842e-02, -4.0349e-02, -7.0771e-02,  3.3271e-02, -5.4562e-02,\n",
       "         -1.2431e-02, -3.8316e-02, -5.4030e-02, -1.3545e-01,  6.7743e-02],\n",
       "        [-8.9069e-02, -3.3747e-02, -6.5021e-02,  3.0535e-02, -5.0706e-02,\n",
       "         -1.0087e-02, -4.2031e-02, -4.7464e-02, -1.4038e-01,  7.8084e-02],\n",
       "        [-9.8270e-02, -4.0277e-02, -6.2311e-02,  1.9713e-02, -6.2959e-02,\n",
       "         -1.2122e-02, -4.3165e-02, -6.1890e-02, -1.3029e-01,  7.0178e-02],\n",
       "        [-9.8115e-02, -4.2436e-02, -6.3373e-02,  1.6547e-02, -6.7274e-02,\n",
       "         -1.3300e-02, -2.7406e-02, -4.3650e-02, -1.3211e-01,  6.2890e-02],\n",
       "        [-9.5664e-02, -3.3644e-02, -6.2878e-02,  2.0487e-02, -6.4903e-02,\n",
       "         -1.2599e-02, -3.9898e-02, -6.2138e-02, -1.3023e-01,  6.2352e-02],\n",
       "        [-9.8870e-02, -4.2176e-02, -7.2430e-02,  2.1615e-02, -6.0828e-02,\n",
       "         -8.9121e-03, -2.4909e-02, -3.9310e-02, -1.4219e-01,  6.2275e-02],\n",
       "        [-9.7074e-02, -3.8492e-02, -6.4014e-02,  3.6568e-02, -5.0315e-02,\n",
       "          6.8752e-03, -4.4102e-02, -5.3695e-02, -1.4879e-01,  6.9547e-02],\n",
       "        [-9.8095e-02, -3.9724e-02, -7.0853e-02,  2.6222e-02, -5.9174e-02,\n",
       "         -1.3611e-02, -3.2946e-02, -4.9858e-02, -1.3237e-01,  6.7092e-02],\n",
       "        [-9.9627e-02, -3.9551e-02, -6.4596e-02,  2.9343e-02, -5.7751e-02,\n",
       "         -2.3757e-03, -6.0358e-02, -5.4396e-02, -1.3351e-01,  7.1068e-02],\n",
       "        [-1.0732e-01, -4.4140e-02, -6.7136e-02,  2.5867e-02, -7.0461e-02,\n",
       "         -8.2711e-03, -4.8628e-02, -5.6936e-02, -1.4007e-01,  6.6848e-02],\n",
       "        [-9.5051e-02, -4.1766e-02, -6.2441e-02,  1.8346e-02, -6.2769e-02,\n",
       "         -8.8535e-03, -4.5887e-02, -5.9188e-02, -1.3352e-01,  7.1154e-02],\n",
       "        [-9.9852e-02, -3.6371e-02, -6.5099e-02,  2.3257e-02, -6.4198e-02,\n",
       "         -1.1684e-02, -4.3230e-02, -5.5880e-02, -1.2814e-01,  6.8610e-02],\n",
       "        [-9.8207e-02, -4.2339e-02, -5.5944e-02,  1.9693e-02, -6.1419e-02,\n",
       "         -1.5288e-02, -4.6408e-02, -5.9546e-02, -1.2380e-01,  7.0461e-02],\n",
       "        [-9.9967e-02, -3.5143e-02, -6.3481e-02,  2.6026e-02, -5.9833e-02,\n",
       "         -9.1950e-03, -4.8925e-02, -6.2124e-02, -1.3131e-01,  6.0961e-02],\n",
       "        [-9.8366e-02, -3.9212e-02, -6.5498e-02,  2.1937e-02, -6.5534e-02,\n",
       "         -7.9806e-03, -3.7927e-02, -5.4312e-02, -1.3949e-01,  6.6722e-02],\n",
       "        [-9.6454e-02, -4.3081e-02, -7.0218e-02,  3.0738e-02, -5.7422e-02,\n",
       "         -1.4677e-02, -4.3313e-02, -5.9480e-02, -1.2745e-01,  6.4290e-02],\n",
       "        [-9.5921e-02, -3.4177e-02, -6.0001e-02,  2.6243e-02, -5.2608e-02,\n",
       "         -6.2035e-04, -4.9031e-02, -5.0407e-02, -1.2682e-01,  6.1795e-02],\n",
       "        [-9.8091e-02, -4.0232e-02, -6.9973e-02,  1.8693e-02, -6.4992e-02,\n",
       "         -1.0641e-02, -3.0504e-02, -4.5821e-02, -1.3519e-01,  6.9660e-02],\n",
       "        [-9.4273e-02, -3.9355e-02, -6.3879e-02,  2.4037e-02, -5.3112e-02,\n",
       "         -1.3130e-02, -3.2278e-02, -4.6585e-02, -1.2941e-01,  7.1105e-02],\n",
       "        [-9.2664e-02, -4.7624e-02, -6.6531e-02,  2.3456e-02, -6.5225e-02,\n",
       "         -8.4880e-03, -5.1649e-02, -5.4320e-02, -1.3322e-01,  7.0962e-02],\n",
       "        [-1.0511e-01, -2.9069e-02, -6.2791e-02,  3.1276e-02, -4.9385e-02,\n",
       "         -4.5754e-03, -4.9865e-02, -6.1143e-02, -1.2910e-01,  6.9230e-02],\n",
       "        [-9.3931e-02, -5.2465e-02, -7.5943e-02,  2.9435e-02, -5.9401e-02,\n",
       "         -1.4125e-02, -4.0286e-02, -5.3657e-02, -1.4185e-01,  6.6425e-02],\n",
       "        [-9.6489e-02, -3.9123e-02, -6.2641e-02,  3.6254e-02, -5.6412e-02,\n",
       "         -5.9433e-03, -4.5560e-02, -5.3786e-02, -1.3819e-01,  6.6410e-02],\n",
       "        [-9.5469e-02, -3.8027e-02, -5.8455e-02,  3.4491e-02, -4.7703e-02,\n",
       "         -2.5191e-03, -4.5408e-02, -5.4664e-02, -1.2981e-01,  6.8140e-02],\n",
       "        [-8.1936e-02, -5.5486e-02, -6.8381e-02,  2.4338e-02, -6.0346e-02,\n",
       "         -1.7282e-02, -3.4369e-02, -5.2832e-02, -1.3737e-01,  7.1650e-02],\n",
       "        [-9.3698e-02, -3.2785e-02, -6.7695e-02,  3.3763e-02, -4.8893e-02,\n",
       "         -9.9952e-03, -2.6677e-02, -5.0980e-02, -1.3719e-01,  6.7905e-02],\n",
       "        [-9.1459e-02, -4.3166e-02, -6.3955e-02,  2.4230e-02, -5.9314e-02,\n",
       "         -9.5932e-03, -4.4600e-02, -4.2361e-02, -1.3024e-01,  6.9382e-02],\n",
       "        [-1.0765e-01, -4.9417e-02, -6.5020e-02,  3.1112e-02, -5.8153e-02,\n",
       "         -3.4356e-03, -4.8898e-02, -5.5197e-02, -1.3637e-01,  7.1473e-02],\n",
       "        [-9.9349e-02, -3.9243e-02, -5.6667e-02,  3.3316e-02, -4.6659e-02,\n",
       "         -1.9457e-03, -3.9562e-02, -4.5609e-02, -1.3521e-01,  7.4266e-02],\n",
       "        [-9.3381e-02, -4.0391e-02, -6.9896e-02,  3.5273e-02, -5.5715e-02,\n",
       "         -5.9332e-03, -4.2958e-02, -5.4417e-02, -1.4015e-01,  6.9547e-02],\n",
       "        [-1.0137e-01, -3.3676e-02, -6.1515e-02,  1.9078e-02, -5.8908e-02,\n",
       "         -1.5122e-02, -4.3736e-02, -6.2029e-02, -1.2334e-01,  6.1229e-02],\n",
       "        [-8.9264e-02, -4.5918e-02, -7.1523e-02,  3.4435e-02, -5.1903e-02,\n",
       "         -1.0291e-02, -3.2623e-02, -4.9329e-02, -1.3837e-01,  6.3164e-02],\n",
       "        [-1.0563e-01, -2.9866e-02, -5.9929e-02,  2.5310e-02, -5.5406e-02,\n",
       "         -1.2570e-02, -4.4702e-02, -5.8907e-02, -1.2826e-01,  6.1549e-02],\n",
       "        [-8.6226e-02, -6.3562e-02, -7.5889e-02,  1.2775e-02, -7.8910e-02,\n",
       "         -1.5763e-02, -3.8349e-02, -4.1191e-02, -1.3870e-01,  6.7971e-02],\n",
       "        [-9.3174e-02, -3.9966e-02, -6.0523e-02,  3.6913e-02, -4.3770e-02,\n",
       "         -2.3082e-03, -4.1542e-02, -5.1214e-02, -1.3716e-01,  7.1737e-02],\n",
       "        [-1.0052e-01, -3.5729e-02, -6.9906e-02,  3.4100e-02, -5.6672e-02,\n",
       "         -6.1355e-03, -4.6479e-02, -5.6871e-02, -1.3494e-01,  6.2298e-02],\n",
       "        [-1.0897e-01, -3.9096e-02, -6.5810e-02,  2.3655e-02, -5.5573e-02,\n",
       "         -1.1883e-02, -4.3737e-02, -5.6082e-02, -1.2936e-01,  6.6127e-02]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F \n",
    "# Define model\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n",
    "        self.pool=nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2=nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n",
    "\n",
    "        self.fc1=nn.Linear(16*4*4, 120)\n",
    "        self.fc2=nn.Linear(120,84)\n",
    "        self.fc3=nn.Linear(84,10)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.pool(F.relu(self.conv1(x)))    \n",
    "        x=self.pool(F.relu(self.conv2(x)))\n",
    "        x=torch.flatten(x, 1)\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "model=NeuralNetwork().to(device)\n",
    "print(model)\n",
    "X=X.to(device)\n",
    "x=X[0].to(device)\n",
    "model(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current Layer Size: 150\n",
      "current Layer Size: 6\n",
      "current Layer Size: 2400\n",
      "current Layer Size: 16\n",
      "current Layer Size: 30720\n",
      "current Layer Size: 120\n",
      "current Layer Size: 10080\n",
      "current Layer Size: 84\n",
      "current Layer Size: 840\n",
      "current Layer Size: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(44426)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "total_params=0\n",
    "for params in model.parameters():\n",
    "    current_layer_size=np.prod(params.shape)\n",
    "    print(\"current Layer Size:\", current_layer_size)\n",
    "    total_params +=current_layer_size\n",
    "\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size=len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # compute prediction error\n",
    "        pred = model(X)\n",
    "        loss=loss_fn(pred,y)\n",
    "\n",
    "        #Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch+1) * len(X)\n",
    "            print(f\"loss:{loss:>7f} [{current:>5d} / {size:>5d}]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size=len(dataloader.dataset)\n",
    "    num_batches=len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss,correct=0,0\n",
    "    with torch.no_grad():\n",
    "        for X,y in dataloader:\n",
    "            X, y=X.to(device), y.to(device)\n",
    "            pred=model(X)\n",
    "            test_loss += loss_fn(pred,y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        test_loss /= num_batches\n",
    "        correct /= size\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg Loss:{test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 10.1%, Avg Loss:2.303866 \n",
      "\n",
      "Epoch1\n",
      " -----------------\n",
      "loss:2.301030 [   64 / 60000]\n",
      "loss:2.314699 [ 6464 / 60000]\n",
      "loss:2.310412 [12864 / 60000]\n",
      "loss:2.290899 [19264 / 60000]\n",
      "loss:2.303945 [25664 / 60000]\n",
      "loss:2.318090 [32064 / 60000]\n",
      "loss:2.306573 [38464 / 60000]\n",
      "loss:2.303086 [44864 / 60000]\n",
      "loss:2.304069 [51264 / 60000]\n",
      "loss:2.294434 [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 10.1%, Avg Loss:2.303866 \n",
      "\n",
      "Epoch2\n",
      " -----------------\n",
      "loss:2.301030 [   64 / 60000]\n",
      "loss:2.314699 [ 6464 / 60000]\n",
      "loss:2.310412 [12864 / 60000]\n",
      "loss:2.290899 [19264 / 60000]\n",
      "loss:2.303945 [25664 / 60000]\n",
      "loss:2.318090 [32064 / 60000]\n",
      "loss:2.306573 [38464 / 60000]\n",
      "loss:2.303086 [44864 / 60000]\n",
      "loss:2.304069 [51264 / 60000]\n",
      "loss:2.294434 [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 10.1%, Avg Loss:2.303866 \n",
      "\n",
      "Epoch3\n",
      " -----------------\n",
      "loss:2.301030 [   64 / 60000]\n",
      "loss:2.314699 [ 6464 / 60000]\n",
      "loss:2.310412 [12864 / 60000]\n",
      "loss:2.290899 [19264 / 60000]\n",
      "loss:2.303945 [25664 / 60000]\n",
      "loss:2.318090 [32064 / 60000]\n",
      "loss:2.306573 [38464 / 60000]\n",
      "loss:2.303086 [44864 / 60000]\n",
      "loss:2.304069 [51264 / 60000]\n",
      "loss:2.294434 [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 10.1%, Avg Loss:2.303866 \n",
      "\n",
      "Epoch4\n",
      " -----------------\n",
      "loss:2.301030 [   64 / 60000]\n",
      "loss:2.314699 [ 6464 / 60000]\n",
      "loss:2.310412 [12864 / 60000]\n",
      "loss:2.290899 [19264 / 60000]\n",
      "loss:2.303945 [25664 / 60000]\n",
      "loss:2.318090 [32064 / 60000]\n",
      "loss:2.306573 [38464 / 60000]\n",
      "loss:2.303086 [44864 / 60000]\n",
      "loss:2.304069 [51264 / 60000]\n",
      "loss:2.294434 [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 10.1%, Avg Loss:2.303866 \n",
      "\n",
      "Epoch5\n",
      " -----------------\n",
      "loss:2.301030 [   64 / 60000]\n",
      "loss:2.314699 [ 6464 / 60000]\n",
      "loss:2.310412 [12864 / 60000]\n",
      "loss:2.290899 [19264 / 60000]\n",
      "loss:2.303945 [25664 / 60000]\n",
      "loss:2.318090 [32064 / 60000]\n",
      "loss:2.306573 [38464 / 60000]\n",
      "loss:2.303086 [44864 / 60000]\n",
      "loss:2.304069 [51264 / 60000]\n",
      "loss:2.294434 [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 10.1%, Avg Loss:2.303866 \n",
      "\n",
      "Epoch6\n",
      " -----------------\n",
      "loss:2.301030 [   64 / 60000]\n",
      "loss:2.314699 [ 6464 / 60000]\n",
      "loss:2.310412 [12864 / 60000]\n",
      "loss:2.290899 [19264 / 60000]\n",
      "loss:2.303945 [25664 / 60000]\n",
      "loss:2.318090 [32064 / 60000]\n",
      "loss:2.306573 [38464 / 60000]\n",
      "loss:2.303086 [44864 / 60000]\n",
      "loss:2.304069 [51264 / 60000]\n",
      "loss:2.294434 [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 10.1%, Avg Loss:2.303866 \n",
      "\n",
      "Epoch7\n",
      " -----------------\n",
      "loss:2.301030 [   64 / 60000]\n",
      "loss:2.314699 [ 6464 / 60000]\n",
      "loss:2.310412 [12864 / 60000]\n",
      "loss:2.290899 [19264 / 60000]\n",
      "loss:2.303945 [25664 / 60000]\n",
      "loss:2.318090 [32064 / 60000]\n",
      "loss:2.306573 [38464 / 60000]\n",
      "loss:2.303086 [44864 / 60000]\n",
      "loss:2.304069 [51264 / 60000]\n",
      "loss:2.294434 [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 10.1%, Avg Loss:2.303866 \n",
      "\n",
      "Epoch8\n",
      " -----------------\n",
      "loss:2.301030 [   64 / 60000]\n",
      "loss:2.314699 [ 6464 / 60000]\n",
      "loss:2.310412 [12864 / 60000]\n",
      "loss:2.290899 [19264 / 60000]\n",
      "loss:2.303945 [25664 / 60000]\n",
      "loss:2.318090 [32064 / 60000]\n",
      "loss:2.306573 [38464 / 60000]\n",
      "loss:2.303086 [44864 / 60000]\n",
      "loss:2.304069 [51264 / 60000]\n",
      "loss:2.294434 [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 10.1%, Avg Loss:2.303866 \n",
      "\n",
      "Epoch9\n",
      " -----------------\n",
      "loss:2.301030 [   64 / 60000]\n",
      "loss:2.314699 [ 6464 / 60000]\n",
      "loss:2.310412 [12864 / 60000]\n",
      "loss:2.290899 [19264 / 60000]\n",
      "loss:2.303945 [25664 / 60000]\n",
      "loss:2.318090 [32064 / 60000]\n",
      "loss:2.306573 [38464 / 60000]\n",
      "loss:2.303086 [44864 / 60000]\n",
      "loss:2.304069 [51264 / 60000]\n",
      "loss:2.294434 [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 10.1%, Avg Loss:2.303866 \n",
      "\n",
      "Epoch10\n",
      " -----------------\n",
      "loss:2.301030 [   64 / 60000]\n",
      "loss:2.314699 [ 6464 / 60000]\n",
      "loss:2.310412 [12864 / 60000]\n",
      "loss:2.290899 [19264 / 60000]\n",
      "loss:2.303945 [25664 / 60000]\n",
      "loss:2.318090 [32064 / 60000]\n",
      "loss:2.306573 [38464 / 60000]\n",
      "loss:2.303086 [44864 / 60000]\n",
      "loss:2.304069 [51264 / 60000]\n",
      "loss:2.294434 [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 10.1%, Avg Loss:2.303866 \n",
      "\n",
      "done in  134.32846927642822 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "epochs = 10\n",
    "test(test_dataloader, model, loss_fn)\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch{t+1}\\n -----------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"done in \",time.time()-start,\"seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
